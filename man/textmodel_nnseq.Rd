% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/textmodel_nnseq.R
\name{textmodel_nnseq}
\alias{textmodel_nnseq}
\title{sequential neural network model for text}
\usage{
textmodel_nnseq(x, y, Seed = 17, Epochs = 3, Units = 512,
  Batch = 32, Dropout = 0.2, Valsplit = 0.1,
  Metric = "categorical_accuracy", Loss = "categorical_crossentropy",
  Optimizer = "adam", Verbose = TRUE, ...)
}
\arguments{
\item{x}{The data that will be used as training and test data.}

\item{y}{The outcome that will be used as outcomes to be predicted by the NN model.}

\item{Seed}{The seed used in the model. Defaults to 17}

\item{Epochs}{The number of epochs used in the NN model.}

\item{Units}{The number of network nodes used in the first layer of the sequential model}

\item{Batch}{The number of batches estimated}

\item{Dropout}{A floating variable bound between 0 and 1. It determines the rate at which units are dropped for the linear tranformation of the inputs.}

\item{Metric}{Metric used to train algorithm}

\item{Loss}{Metric used to train algorithm}

\item{Optimizer}{Optimizer used to fit model to training data}

\item{...}{additional options passed to
\code{\link[keras]{fit.keras.engine.training.Model}}}

\item{units}{The number of network nodes used in the first layer of the
sequential model}

\item{dropout}{A floating variable bound between 0 and 1. It determines the
rate at which units are dropped for the linear transformation of the
inputs.}

\item{optimizer}{optimizer used to fit model to training data, see
\code{\link[keras]{compile.keras.engine.training.Model}}}

\item{loss}{objective loss function, see
\code{\link[keras]{compile.keras.engine.training.Model}}}

\item{metrics}{metric used to train algorithm, see
\code{\link[keras]{compile.keras.engine.training.Model}}}
}
\description{
This function is a wrapper for a sequential neural network model with a single hidden layer
network with two layers, implemented in the \pkg{keras} package.
}
\examples{
# need examples here
}
\keyword{textmodel}
