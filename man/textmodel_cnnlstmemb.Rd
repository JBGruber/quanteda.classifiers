% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/textmodel_cnnlstmemb.R
\name{textmodel_cnnlstmemb}
\alias{textmodel_cnnlstmemb}
\title{sequential neural network model for text classification}
\usage{
textmodel_cnnlstmemb(x, y, units = 512, dropout1 = 0.2,
  dropout2 = 0.2, dropout3 = 0.2, wordembeddim = 30, filter = 48,
  kernel_size = 5, pool_size = 4, units_lstm = 128, words = NULL,
  maxsenlen = 50, optimizer = "adam",
  loss = "categorical_crossentropy", metrics = "categorical_accuracy",
  ...)
}
\arguments{
\item{x}{tokens object}

\item{y}{vector of training labels associated with each document identified 
in \code{train}.  (These will be converted to factors if not already 
factors.)}

\item{units}{The number of network nodes used in the first layer of the
sequential model}

\item{dropout1}{A floating variable bound between 0 and 1. It determines the
rate at which units are dropped for the linear transformation of the
inputs for the embedding layer.}

\item{dropout2}{A floating variable bound between 0 and 1. It determines the
rate at which units are dropped for the linear transformation of the
inputs for the LSTM layer.}

\item{dropout3}{A floating variable bound between 0 and 1. It determines the
rate at which units are dropped for the linear transformation of the
inputs for the recurrent layer.}

\item{wordembeddim}{The number of word embedding dimensions to be fit}

\item{filter}{The number of output filters in the convolution}

\item{kernel_size}{An integer or list of a single integer, specifying the length of the 1D convolution window}

\item{pool_size}{Size of the max pooling windows.
\code{\link[keras]{layer_max_pooling_1d}}}

\item{units_lstm}{The number of nodes of the lstm layer}

\item{words}{The maximum number of words used to train model. Defaults to the number of features in \code{x}}

\item{maxsenlen}{The maximum sentence length of training data}

\item{optimizer}{optimizer used to fit model to training data, see
\code{\link[keras]{compile.keras.engine.training.Model}}}

\item{loss}{objective loss function, see
\code{\link[keras]{compile.keras.engine.training.Model}}}

\item{metrics}{metric used to train algorithm, see
\code{\link[keras]{compile.keras.engine.training.Model}}}

\item{...}{additional options passed to
\code{\link[keras]{fit.keras.engine.training.Model}}}
}
\description{
Explain
}
\examples{
\dontrun{
# create a dataset with evenly balanced coded and uncoded immigration sentences
corp <- corpus_subset(data_corpus_manifestosentsUK, !is.na(crowd_immigration_label))

test <- sample(1:ndoc(corpcoded), size = ndoc(corpcoded) * .2)

corptrain <- tokens(texts(corpcoded)[-test])
ytrain <- docvars(corpcoded, "crowd_immigration_label")[-test]

corptest <- tokens(texts(corpcoded)[test])
ytest <- docvars(corpcoded, "crowd_immigration_label")[test]

tmod <- textmodel_cnnlstmemb(corptrain, y = ytrain,
                             epochs = 5, verbose = 1)

pred <- predict(tmod, newdata = corptest)
table(ytest, pred)

}
}
\keyword{textmodel}
